#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
500ç« ç´¢å¼•ç³»ç»Ÿå‹åŠ›æµ‹è¯•

æµ‹è¯•ç›®æ ‡ï¼š
1. index.db å¤§å°å¢é•¿æ›²çº¿
2. å®ä½“åŒæ­¥æ€§èƒ½ï¼ˆentities_v3 â†’ index.dbï¼‰
3. åˆ«åæŸ¥è¯¢æ€§èƒ½
4. æ¨¡ç³Šæœç´¢æ€§èƒ½
5. ä¼ç¬”ç´§æ€¥åº¦è®¡ç®—æ€§èƒ½
6. å…³ç³»å›¾æŸ¥è¯¢æ€§èƒ½
7. å¹¶å‘è¯»å†™ç¨³å®šæ€§

ä¾èµ–ï¼šstress_test_500chapters.py ç”Ÿæˆçš„ state.json
"""

import json
import os
import sys
import time
import random
import sqlite3
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(script_dir))

from security_utils import atomic_write_json, read_json_safe

# Windows ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


# ============================================================================
# æ¨¡æ‹Ÿé…ç½®ï¼ˆä¸ stress_test_500chapters.py ä¿æŒä¸€è‡´ï¼‰
# ============================================================================

CONFIG = {
    "total_chapters": 500,
    "words_per_chapter": 3500,
    "new_character_base_rate": 0.8,
    "new_character_decay": 0.95,
    "new_location_rate": 0.3,
    "new_item_rate": 0.2,
    "foreshadow_plant_rate": 0.5,
    "foreshadow_resolve_rate": 0.3,
    "relationship_update_interval": 5,
}

SURNAME_POOL = ["æ—", "é™ˆ", "ç‹", "æ", "å¼ ", "åˆ˜", "èµµ", "é»„", "å‘¨", "å´", "å¾", "å­™", "é©¬", "æœ±", "èƒ¡", "éƒ­", "ä½•", "é«˜", "ç½—", "éƒ‘"]
NAME_POOL = ["å¤©", "äº‘", "é£", "é›·", "ç«", "æ°´", "æœˆ", "æ˜Ÿ", "é¾™", "å‡¤", "è™", "é¹¤", "å‰‘", "åˆ€", "æª", "æ£", "æ‹³", "æŒ", "æŒ‡", "å¿ƒ"]


class IndexMetrics:
    """ç´¢å¼•æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.checkpoints: List[Dict] = []
        self.sync_times: List[float] = []
        self.query_times: Dict[str, List[float]] = {
            "alias_lookup": [],
            "fuzzy_search": [],
            "foreshadow_urgency": [],
            "relationship_query": [],
            "entity_by_type": [],
        }
        self.errors: List[str] = []

    def record_checkpoint(self, chapter: int, db_path: Path, state: Dict):
        """è®°å½•æ£€æŸ¥ç‚¹"""
        db_size = db_path.stat().st_size if db_path.exists() else 0

        # ç»Ÿè®¡å„è¡¨è¡Œæ•°
        table_counts = {}
        if db_path.exists():
            try:
                conn = sqlite3.connect(str(db_path))
                cursor = conn.cursor()
                for table in ["chapters", "entities", "entity_aliases", "entity_kv",
                              "entity_history", "foreshadowing_index", "relationships"]:
                    try:
                        cursor.execute(f"SELECT COUNT(*) FROM {table}")
                        table_counts[table] = cursor.fetchone()[0]
                    except sqlite3.OperationalError:
                        table_counts[table] = 0
                conn.close()
            except Exception as e:
                self.errors.append(f"DB stats error: {e}")

        self.checkpoints.append({
            "chapter": chapter,
            "db_size_kb": db_size / 1024,
            "table_counts": table_counts,
            "avg_sync_time_ms": sum(self.sync_times[-10:]) / max(len(self.sync_times[-10:]), 1) * 1000,
            "query_performance": {
                k: sum(v[-10:]) / max(len(v[-10:]), 1) * 1000
                for k, v in self.query_times.items()
            }
        })

    def record_sync_time(self, duration: float):
        self.sync_times.append(duration)

    def record_query_time(self, query_type: str, duration: float):
        if query_type in self.query_times:
            self.query_times[query_type].append(duration)

    def record_error(self, error: str):
        self.errors.append(error)

    def generate_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.checkpoints:
            return "No data collected"

        final = self.checkpoints[-1]
        first = self.checkpoints[0] if self.checkpoints else final

        lines = [
            "=" * 70,
            "ğŸ“Š 500ç« ç´¢å¼•ç³»ç»Ÿå‹åŠ›æµ‹è¯•æŠ¥å‘Š",
            "=" * 70,
            "",
            "## index.db å¢é•¿",
            f"- åˆå§‹å¤§å°: {first['db_size_kb']:.2f} KB",
            f"- æœ€ç»ˆå¤§å°: {final['db_size_kb']:.2f} KB",
            f"- å¢é•¿å€æ•°: {final['db_size_kb'] / max(first['db_size_kb'], 0.1):.1f}x",
            "",
            "## è¡¨è¡Œæ•°ç»Ÿè®¡",
        ]

        for table, count in final.get('table_counts', {}).items():
            lines.append(f"  - {table}: {count:,}")

        lines.extend([
            "",
            "## åŒæ­¥æ€§èƒ½",
            f"- å¹³å‡åŒæ­¥æ—¶é—´: {sum(self.sync_times) / max(len(self.sync_times), 1) * 1000:.2f} ms",
            f"- æœ€å¤§åŒæ­¥æ—¶é—´: {max(self.sync_times) * 1000:.2f} ms" if self.sync_times else "N/A",
            f"- æœ€å°åŒæ­¥æ—¶é—´: {min(self.sync_times) * 1000:.2f} ms" if self.sync_times else "N/A",
            "",
            "## æŸ¥è¯¢æ€§èƒ½ï¼ˆå¹³å‡ï¼‰",
        ])

        for query_type, times in self.query_times.items():
            if times:
                avg = sum(times) / len(times) * 1000
                lines.append(f"  - {query_type}: {avg:.2f} ms")

        lines.extend([
            "",
            "## é”™è¯¯ç»Ÿè®¡",
            f"- é”™è¯¯æ•°: {len(self.errors)}",
        ])

        if self.errors:
            lines.append("- é”™è¯¯è¯¦æƒ…:")
            for err in self.errors[:10]:
                lines.append(f"  - {err[:80]}")

        # å¢é•¿æ›²çº¿
        lines.extend([
            "",
            "## å¢é•¿æ›²çº¿ï¼ˆæ¯100ç« ï¼‰",
            "| ç« èŠ‚ | DBå¤§å°(KB) | entities | aliases | foreshadow | åŒæ­¥(ms) |",
            "|------|-----------|----------|---------|------------|----------|",
        ])

        for cp in self.checkpoints:
            if cp['chapter'] % 100 == 0 or cp['chapter'] == final['chapter']:
                tc = cp.get('table_counts', {})
                lines.append(
                    f"| {cp['chapter']} | {cp['db_size_kb']:.1f} | "
                    f"{tc.get('entities', 0)} | {tc.get('entity_aliases', 0)} | "
                    f"{tc.get('foreshadowing_index', 0)} | {cp['avg_sync_time_ms']:.1f} |"
                )

        # æŸ¥è¯¢æ€§èƒ½è¶‹åŠ¿
        lines.extend([
            "",
            "## æŸ¥è¯¢æ€§èƒ½è¶‹åŠ¿ï¼ˆæ¯100ç« ï¼‰",
            "| ç« èŠ‚ | aliasæŸ¥è¯¢(ms) | æ¨¡ç³Šæœç´¢(ms) | ä¼ç¬”ç´§æ€¥åº¦(ms) | å…³ç³»æŸ¥è¯¢(ms) |",
            "|------|--------------|-------------|---------------|-------------|",
        ])

        for cp in self.checkpoints:
            if cp['chapter'] % 100 == 0 or cp['chapter'] == final['chapter']:
                qp = cp.get('query_performance', {})
                lines.append(
                    f"| {cp['chapter']} | {qp.get('alias_lookup', 0):.2f} | "
                    f"{qp.get('fuzzy_search', 0):.2f} | "
                    f"{qp.get('foreshadow_urgency', 0):.2f} | "
                    f"{qp.get('relationship_query', 0):.2f} |"
                )

        # ç¨³å®šæ€§è¯„ä¼°
        lines.extend([
            "",
            "## ç¨³å®šæ€§è¯„ä¼°",
        ])

        if final['db_size_kb'] < 1024:
            lines.append("âœ… æ•°æ®åº“å¤§å°åˆç† (< 1MB)")
        elif final['db_size_kb'] < 5120:
            lines.append("âš ï¸ æ•°æ®åº“åå¤§ (1-5MB)")
        else:
            lines.append("âŒ æ•°æ®åº“è¿‡å¤§ (> 5MB)")

        avg_sync = sum(self.sync_times) / max(len(self.sync_times), 1) * 1000
        if avg_sync < 100:
            lines.append("âœ… åŒæ­¥æ€§èƒ½è‰¯å¥½ (< 100ms)")
        elif avg_sync < 500:
            lines.append("âš ï¸ åŒæ­¥æ€§èƒ½ä¸€èˆ¬ (100-500ms)")
        else:
            lines.append("âŒ åŒæ­¥æ€§èƒ½å·® (> 500ms)")

        # æŸ¥è¯¢æ€§èƒ½è¯„ä¼°
        for query_type, times in self.query_times.items():
            if times:
                avg = sum(times) / len(times) * 1000
                if avg < 10:
                    lines.append(f"âœ… {query_type} æŸ¥è¯¢å¿«é€Ÿ (< 10ms)")
                elif avg < 50:
                    lines.append(f"âš ï¸ {query_type} æŸ¥è¯¢ä¸€èˆ¬ (10-50ms)")
                else:
                    lines.append(f"âŒ {query_type} æŸ¥è¯¢æ…¢ (> 50ms)")

        if not self.errors:
            lines.append("âœ… æ— é”™è¯¯")
        else:
            lines.append(f"âŒ æœ‰ {len(self.errors)} ä¸ªé”™è¯¯")

        lines.append("")
        lines.append("=" * 70)

        return "\n".join(lines)


class IndexSimulator:
    """ç´¢å¼•ç³»ç»Ÿæ¨¡æ‹Ÿå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.state_file = project_root / ".webnovel" / "state.json"
        self.db_path = project_root / ".webnovel" / "index.db"
        self.metrics = IndexMetrics()
        self.generated_names = set()
        self.entity_id_counter = 0

    def _generate_id(self, prefix: str) -> str:
        self.entity_id_counter += 1
        return f"{prefix}_{self.entity_id_counter:05d}"

    def _generate_character_name(self) -> str:
        for _ in range(100):
            name = random.choice(SURNAME_POOL) + random.choice(NAME_POOL) + random.choice(NAME_POOL)
            if name not in self.generated_names:
                self.generated_names.add(name)
                return name
        return f"è§’è‰²_{len(self.generated_names)}"

    def _get_character_rate(self, chapter: int) -> float:
        decay_periods = chapter // 50
        rate = CONFIG["new_character_base_rate"] * (CONFIG["new_character_decay"] ** decay_periods)
        return max(rate, 0.1)

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        # åˆ›å»ºè¡¨ç»“æ„ï¼ˆä¸ structured_index.py ä¸€è‡´ï¼‰
        cursor.executescript("""
            -- ç« èŠ‚è¡¨
            CREATE TABLE IF NOT EXISTS chapters (
                chapter_num INTEGER PRIMARY KEY,
                title TEXT,
                word_count INTEGER,
                summary TEXT,
                main_location TEXT,
                characters TEXT,
                content_hash TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- å®ä½“ä¸»è¡¨
            CREATE TABLE IF NOT EXISTS entities (
                entity_id TEXT PRIMARY KEY,
                entity_type TEXT NOT NULL,
                canonical_name TEXT,
                tier TEXT,
                desc TEXT,
                created_chapter INTEGER,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- åˆ«åè¡¨
            CREATE TABLE IF NOT EXISTS entity_aliases (
                alias TEXT,
                entity_id TEXT,
                entity_type TEXT,
                first_seen_chapter INTEGER,
                context TEXT,
                PRIMARY KEY (alias, entity_id)
            );
            CREATE INDEX IF NOT EXISTS idx_alias ON entity_aliases(alias);

            -- å®ä½“å±æ€§ (KV)
            CREATE TABLE IF NOT EXISTS entity_kv (
                entity_id TEXT,
                key TEXT,
                value TEXT,
                last_chapter INTEGER,
                PRIMARY KEY (entity_id, key)
            );

            -- å®ä½“å†å²
            CREATE TABLE IF NOT EXISTS entity_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                entity_id TEXT,
                chapter INTEGER,
                changes_json TEXT,
                reasons_json TEXT,
                added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- ä¼ç¬”ç´¢å¼•
            CREATE TABLE IF NOT EXISTS foreshadowing_index (
                foreshadow_id TEXT PRIMARY KEY,
                content TEXT,
                tier TEXT,
                status TEXT,
                planted_chapter INTEGER,
                target_chapter INTEGER,
                resolved_chapter INTEGER,
                urgency_score REAL
            );

            -- å…³ç³»è¡¨
            CREATE TABLE IF NOT EXISTS relationships (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                char1_id TEXT,
                char2_id TEXT,
                rel_type TEXT,
                intensity INTEGER,
                established_chapter INTEGER,
                description TEXT
            );
            CREATE INDEX IF NOT EXISTS idx_rel_char1 ON relationships(char1_id);
            CREATE INDEX IF NOT EXISTS idx_rel_char2 ON relationships(char2_id);
        """)

        conn.commit()
        conn.close()

    def init_project(self):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿé¡¹ç›®"""
        self.project_root.mkdir(parents=True, exist_ok=True)
        (self.project_root / ".webnovel").mkdir(exist_ok=True)

        # åˆå§‹ state.json
        initial_state = {
            "project_info": {"title": "ç´¢å¼•æµ‹è¯•å°è¯´", "genre": "ç„å¹»"},
            "progress": {"current_chapter": 0, "total_words": 0},
            "protagonist_state": {"name": "æ—å¤©", "realm": "ç»ƒæ°”", "layer": 1},
            "entities_v3": {"è§’è‰²": {}, "åœ°ç‚¹": {}, "ç‰©å“": {}, "åŠ¿åŠ›": {}, "æ‹›å¼": {}},
            "alias_index": {},
            "foreshadowing": [],
            "relationships": [],
        }

        # æ·»åŠ ä¸»è§’
        protagonist_id = "protagonist_lintian"
        initial_state["entities_v3"]["è§’è‰²"][protagonist_id] = {
            "canonical_name": "æ—å¤©",
            "desc": "ä¸»è§’",
            "tier": "æ ¸å¿ƒ",
            "aliases": ["æ—å¤©", "å¤©å“¥"],
            "current": {"realm": "ç»ƒæ°”"},
            "history": [],
        }
        initial_state["alias_index"]["æ—å¤©"] = [{"type": "è§’è‰²", "id": protagonist_id}]

        atomic_write_json(self.state_file, initial_state, backup=False)
        self.init_database()
        return initial_state

    def sync_to_index(self, state: Dict, chapter: int):
        """åŒæ­¥ state.json åˆ° index.db"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        try:
            # åŒæ­¥ç« èŠ‚
            cursor.execute("""
                INSERT OR REPLACE INTO chapters
                (chapter_num, title, word_count, summary)
                VALUES (?, ?, ?, ?)
            """, (chapter, f"ç¬¬{chapter}ç« ", CONFIG["words_per_chapter"], f"ç¬¬{chapter}ç« æ‘˜è¦"))

            # åŒæ­¥å®ä½“
            entities_v3 = state.get("entities_v3", {})
            for entity_type, entities in entities_v3.items():
                for entity_id, entity_data in entities.items():
                    cursor.execute("""
                        INSERT OR REPLACE INTO entities
                        (entity_id, entity_type, canonical_name, tier, desc, created_chapter)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        entity_id,
                        entity_type,
                        entity_data.get("canonical_name", ""),
                        entity_data.get("tier", "è£…é¥°"),
                        entity_data.get("desc", ""),
                        chapter
                    ))

                    # åŒæ­¥åˆ«å
                    for alias in entity_data.get("aliases", []):
                        cursor.execute("""
                            INSERT OR IGNORE INTO entity_aliases
                            (alias, entity_id, entity_type, first_seen_chapter)
                            VALUES (?, ?, ?, ?)
                        """, (alias, entity_id, entity_type, chapter))

                    # åŒæ­¥å½“å‰å±æ€§
                    for key, value in entity_data.get("current", {}).items():
                        cursor.execute("""
                            INSERT OR REPLACE INTO entity_kv
                            (entity_id, key, value, last_chapter)
                            VALUES (?, ?, ?, ?)
                        """, (entity_id, key, str(value), chapter))

            # åŒæ­¥ä¼ç¬”
            for fs in state.get("foreshadowing", []):
                # è®¡ç®—ç´§æ€¥åº¦
                if fs.get("status") == "æœªå›æ”¶":
                    target = fs.get("target_chapter", chapter + 100)
                    urgency = max(0, 100 - (target - chapter))
                else:
                    urgency = 0

                cursor.execute("""
                    INSERT OR REPLACE INTO foreshadowing_index
                    (foreshadow_id, content, tier, status, planted_chapter,
                     target_chapter, resolved_chapter, urgency_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    fs.get("id", f"fs_{chapter}"),
                    fs.get("content", ""),
                    fs.get("tier", "è£…é¥°"),
                    fs.get("status", "æœªå›æ”¶"),
                    fs.get("planted_chapter", chapter),
                    fs.get("target_chapter"),
                    fs.get("resolved_chapter"),
                    urgency
                ))

            # åŒæ­¥å…³ç³»ï¼ˆä½¿ç”¨ REPLACE é¿å…é‡å¤ï¼‰
            # å…ˆæ¸…ç©ºå†é‡å»ºï¼ˆç®€åŒ–ç­–ç•¥ï¼Œå®é™…ç”Ÿäº§åº”å¢é‡åŒæ­¥ï¼‰
            cursor.execute("DELETE FROM relationships WHERE established_chapter <= ?", (chapter,))
            for rel in state.get("relationships", []):
                cursor.execute("""
                    INSERT INTO relationships
                    (char1_id, char2_id, rel_type, intensity, established_chapter)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    rel.get("char1_id", ""),
                    rel.get("char2_id", ""),
                    rel.get("type", "ally"),
                    rel.get("intensity", 50),
                    rel.get("established_chapter", chapter)
                ))

            conn.commit()

        finally:
            conn.close()

    def run_queries(self, state: Dict, chapter: int):
        """æ‰§è¡Œå„ç±»æŸ¥è¯¢å¹¶è®¡æ—¶"""
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        try:
            # 1. åˆ«åæŸ¥è¯¢
            alias_list = list(state.get("alias_index", {}).keys())
            if alias_list:
                test_alias = random.choice(alias_list)
                start = time.time()
                cursor.execute("SELECT entity_id, entity_type FROM entity_aliases WHERE alias = ?", (test_alias,))
                cursor.fetchall()
                self.metrics.record_query_time("alias_lookup", time.time() - start)

            # 2. æ¨¡ç³Šæœç´¢
            if alias_list:
                search_term = random.choice(alias_list)[:2]  # å–å‰ä¸¤ä¸ªå­—
                start = time.time()
                cursor.execute("""
                    SELECT DISTINCT entity_id, entity_type, alias
                    FROM entity_aliases
                    WHERE alias LIKE ?
                    LIMIT 20
                """, (f"%{search_term}%",))
                cursor.fetchall()
                self.metrics.record_query_time("fuzzy_search", time.time() - start)

            # 3. ä¼ç¬”ç´§æ€¥åº¦æŸ¥è¯¢
            start = time.time()
            cursor.execute("""
                SELECT foreshadow_id, content, urgency_score
                FROM foreshadowing_index
                WHERE status = 'æœªå›æ”¶'
                ORDER BY urgency_score DESC
                LIMIT 10
            """)
            cursor.fetchall()
            self.metrics.record_query_time("foreshadow_urgency", time.time() - start)

            # 4. å…³ç³»æŸ¥è¯¢
            entities_v3 = state.get("entities_v3", {})
            char_ids = list(entities_v3.get("è§’è‰²", {}).keys())
            if char_ids:
                test_char = random.choice(char_ids)
                start = time.time()
                cursor.execute("""
                    SELECT char2_id, rel_type, intensity
                    FROM relationships
                    WHERE char1_id = ?
                    UNION
                    SELECT char1_id, rel_type, intensity
                    FROM relationships
                    WHERE char2_id = ?
                """, (test_char, test_char))
                cursor.fetchall()
                self.metrics.record_query_time("relationship_query", time.time() - start)

            # 5. æŒ‰ç±»å‹æŸ¥è¯¢å®ä½“
            start = time.time()
            cursor.execute("""
                SELECT entity_id, canonical_name, tier
                FROM entities
                WHERE entity_type = 'è§’è‰²' AND tier = 'æ ¸å¿ƒ'
            """)
            cursor.fetchall()
            self.metrics.record_query_time("entity_by_type", time.time() - start)

        finally:
            conn.close()

    def simulate_chapter(self, chapter: int, state: Dict) -> Dict:
        """æ¨¡æ‹Ÿä¸€ç« çš„æ•°æ®å˜åŒ–ï¼ˆä¸ä¸»æµ‹è¯•è„šæœ¬ç±»ä¼¼ï¼‰"""
        state["progress"]["current_chapter"] = chapter
        state["progress"]["total_words"] += CONFIG["words_per_chapter"]

        entities_v3 = state["entities_v3"]
        alias_index = state["alias_index"]

        # æ–°å¢è§’è‰²
        if random.random() < self._get_character_rate(chapter):
            char_name = self._generate_character_name()
            char_id = self._generate_id("char")
            tier = random.choices(["æ ¸å¿ƒ", "æ”¯çº¿", "è£…é¥°"], weights=[0.1, 0.3, 0.6])[0]

            entities_v3["è§’è‰²"][char_id] = {
                "canonical_name": char_name,
                "desc": f"ç¬¬{chapter}ç« å‡ºåœº",
                "tier": tier,
                "aliases": [char_name],
                "current": {"first_appearance": chapter},
                "history": [],
            }
            alias_index[char_name] = [{"type": "è§’è‰²", "id": char_id}]

            # é¢å¤–åˆ«å
            if random.random() < 0.5:
                alias = char_name[0] + "å…„"
                entities_v3["è§’è‰²"][char_id]["aliases"].append(alias)
                if alias not in alias_index:
                    alias_index[alias] = []
                alias_index[alias].append({"type": "è§’è‰²", "id": char_id})

        # æ–°å¢åœ°ç‚¹
        if random.random() < CONFIG["new_location_rate"]:
            loc_name = random.choice(["å¤©", "äº‘", "é¾™"]) + random.choice(["å±±", "è°·", "åŸ"])
            loc_id = self._generate_id("loc")
            entities_v3["åœ°ç‚¹"][loc_id] = {
                "canonical_name": loc_name,
                "desc": f"ç¬¬{chapter}ç« ",
                "tier": "è£…é¥°",
                "aliases": [loc_name],
                "current": {},
                "history": [],
            }
            if loc_name not in alias_index:
                alias_index[loc_name] = []
            alias_index[loc_name].append({"type": "åœ°ç‚¹", "id": loc_id})

        # ä¼ç¬”
        if random.random() < CONFIG["foreshadow_plant_rate"]:
            state["foreshadowing"].append({
                "id": f"fs_{chapter}_{random.randint(1000, 9999)}",
                "content": f"ç¬¬{chapter}ç« ä¼ç¬”",
                "tier": random.choice(["æ ¸å¿ƒ", "æ”¯çº¿", "è£…é¥°"]),
                "status": "æœªå›æ”¶",
                "planted_chapter": chapter,
                "target_chapter": chapter + random.randint(10, 100),
            })

        # å›æ”¶ä¼ç¬”
        for fs in state["foreshadowing"]:
            if (fs.get("status") == "æœªå›æ”¶" and
                fs.get("target_chapter", 999) <= chapter and
                random.random() < CONFIG["foreshadow_resolve_rate"]):
                fs["status"] = "å·²å›æ”¶"
                fs["resolved_chapter"] = chapter

        # å…³ç³»
        if chapter % CONFIG["relationship_update_interval"] == 0:
            char_ids = list(entities_v3["è§’è‰²"].keys())
            if len(char_ids) >= 2:
                char1, char2 = random.sample(char_ids, 2)
                state["relationships"].append({
                    "char1_id": char1,
                    "char2_id": char2,
                    "type": random.choice(["ally", "enemy", "romance", "rival"]),
                    "intensity": random.randint(30, 100),
                    "established_chapter": chapter,
                })

        return state

    def run_simulation(self, checkpoint_interval: int = 10):
        """è¿è¡Œå®Œæ•´æ¨¡æ‹Ÿ"""
        print("ğŸš€ å¼€å§‹500ç« ç´¢å¼•ç³»ç»Ÿå‹åŠ›æµ‹è¯•...")
        print(f"ğŸ“ æµ‹è¯•ç›®å½•: {self.project_root}")
        print()

        state = self.init_project()
        self.metrics.record_checkpoint(0, self.db_path, state)

        start_time = time.time()

        for chapter in range(1, CONFIG["total_chapters"] + 1):
            try:
                # æ¨¡æ‹Ÿç« èŠ‚æ•°æ®
                state = self.simulate_chapter(chapter, state)

                # ä¿å­˜ state.json
                atomic_write_json(self.state_file, state, use_lock=True, backup=False)

                # åŒæ­¥åˆ°ç´¢å¼•
                sync_start = time.time()
                self.sync_to_index(state, chapter)
                sync_duration = time.time() - sync_start
                self.metrics.record_sync_time(sync_duration)

                # æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•
                self.run_queries(state, chapter)

                # è®°å½•æ£€æŸ¥ç‚¹
                if chapter % checkpoint_interval == 0:
                    self.metrics.record_checkpoint(chapter, self.db_path, state)
                    elapsed = time.time() - start_time
                    eta = elapsed / chapter * (CONFIG["total_chapters"] - chapter)
                    db_size = self.db_path.stat().st_size / 1024 if self.db_path.exists() else 0
                    print(f"  ç¬¬ {chapter:3d} ç«  | "
                          f"DB {db_size:.1f}KB | "
                          f"åŒæ­¥ {sync_duration*1000:.1f}ms | "
                          f"ETA {eta:.0f}s")

            except Exception as e:
                self.metrics.record_error(f"Chapter {chapter}: {str(e)}")
                print(f"  âŒ ç¬¬ {chapter} ç« é”™è¯¯: {e}")

        # æœ€ç»ˆæ£€æŸ¥ç‚¹
        self.metrics.record_checkpoint(CONFIG["total_chapters"], self.db_path, state)

        total_time = time.time() - start_time
        print()
        print(f"âœ… ç´¢å¼•æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f}s")
        print()

        return self.metrics.generate_report()


def main():
    """ä¸»å‡½æ•°"""
    test_dir = Path(tempfile.mkdtemp(prefix="webnovel_index_test_"))

    try:
        simulator = IndexSimulator(test_dir)
        report = simulator.run_simulation(checkpoint_interval=10)

        print(report)

        # ä¿å­˜æŠ¥å‘Š
        report_file = test_dir / "index_stress_test_report.md"
        report_file.write_text(report, encoding="utf-8")
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print(f"\næµ‹è¯•æ•°æ®ç›®å½•: {test_dir}")

    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
